---
title: "MM-Ridge-test"
author: "Matias Salibian-Barrera"
date: "September 13, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Comparing S and MM-Ridge implementations

Below I compare the implementations of the S-ridge estimator
that are present in the packages `mmlasso` and `pense`. In addition
I also look at the output of the MM-ridge estimator computed with 
the function `pense::mstep()` setting `alpha = 0`. 

### TL;DR
The main conclusions seem to be that

- `mmlasso::sridge()` and `pense::pense()` behave similarly enough (at least when `alpha = 0`), but are not identical, and can be rather different (see the repetition of the example below but with `n = 500` instead, for example). This difference does not seem to dissappear with larger samples.
- the residual scale returned by `mmlasso::sridge()` can be rather different from the one in `pense::pense()` (**Can this have an effect on the resulting PENSE-M we use in our simulations?**); and
- the function `pense::mstep()` returns yet another residual scale estimate, this doesn't seem to be documented. Furthermore, for `n = 500` this difference seems to dissappear. 
- the consensus seems to be that it is better to use `pense::pense()` to compute the S- and MM-ridge
estimators.
- furthermore, we should use `pense::pense(alpha=0)` and `pense::mstep(alpha=1)` to compute the MMLASSO
estimator. 

## S-ridge in packages mmlasso and pense

<!-- Below is a comparison (aka "sanity check") between the implementations -->
<!-- of the S-ridge estimators in packages `mmlasso` and `pense`. In addition, -->
<!-- I also look at the output of the MM-ridge estimator computed with  -->
<!-- the function `pense::mstep()` setting `alpha = 0`.  -->

We first load the libraries and generate a simple synthetic data set:
```{r packages, message=FALSE, warning=FALSE}
library(pense)
library(mmlasso)
# simple synthetic example
n <- 100
p <- 20
set.seed(123)
x <- matrix(rnorm(n*p), n, p)
y <- as.vector( x %*% c(rep(2, 5), rep(0, p-5))) + rnorm(n, sd=.5)
```
We now use `mmlasso::sridge()` and `pense::pense(alpha=0)` to 
compute to "versions" of an S-ridge estimator:
```{r sridge}
# mmlasso S-ridge
a <- sridge(x=x, y=y, cualcv.S=5, numlam.S=30, niter.S=50, normin=0,
            denormout=0, alone=1, ncores=4)
# pense S-ridge 
b0 <- pense(X=x, y=y, alpha=0, standardize=TRUE, lambda=1e-9, initial='cold',
           options=pense_options(delta=a$delta))
```
Note that the optimal value of the penalization found by `mmlasso::sridge` is
`r a$lamda` but `pense::pense()` does not accept `lambda=0` as an argument, so 
I used `lambda = 1e-9` above. 

Also note that I did set `options=pense_options(delta=a$delta)` above to make sure
`pense::pense()` was optimizing the same M-scale as `mmlasso::sridge()`. This value
is adjusted internally, and for this example it was equal to `r a$delta`.

Although the estimated residual scales are somewhat different
(`r c(a$scale, b0$scale)`, for `sridge` and `pense`, respectively), the
regression estimators are similar:
```{r regr}
cbind(a$coef, as.vector(b0$coef[,1]))
```

We can now use `pense::mstep()` to do the M-step starting from the S-ridge
estimator as computed by `pense::pense()`:
```{r mmridge}
g <- mstep(b0, complete_grid=TRUE)
cbind(a$coef, as.vector(b0$coef[,1]), g$coefficients[,1])
```
This looks reasonable, however the scale estimators can be a bit different:
```{r sigmas}
c(a$scale, b0$scale, g$scale)
```
Just for the record, the optimal value of the penalization found in this M-step
was
```{r optlam}
g$lambda
```

## An example where the penalization level is not zero


```{r ex4}
# simple synthetic example
n <- 80
p <- 50
set.seed(123)
x <- matrix(rnorm(n*p), n, p)
y <- as.vector( x %*% c(rep(7, 5), rep(0, p-5))) + rnorm(n, sd=.5)
```

We now use `mmlasso::sridge` and `pense::pense` (the latter with `alpha= 0`) to 
compute an S-ridge estimator.
```{r sridge4}
# mmlasso S-ridge
a <- sridge(x=x, y=y, cualcv.S=5, numlam.S=30, niter.S=50, normin=0,
            denormout=0, alone=1, ncores=4)
(a$lamba)
```
And the `pense` version
```{r penseridge4}
# pense S-ridge 
b0 <- pense(X=x, y=y, alpha=0, standardize=TRUE, lambda=a$lamba, initial='cold',
           options=pense_options(delta=a$delta))
```
Note that the optimal value of the penalization found by `mmlasso::sridge` is
`r a$lamda`. Also note that I did set `options=pense_options(delta=a$delta)`, 
which has the value `r a$delta`, 
to make sure `pense::pense()` was optimizing the same M-scale as `mmlasso::sridge()`. 

The estimated residual scales are 
`r c(a$scale, b0$scale)`, for `sridge` and `pense`, respectively, and the
regression estimators are:
```{r regr4}
cbind(a$coef, as.vector(b0$coef[,1]))
```

We can now use `pense::mstep()` to do the M-step starting from the S-ridge
estimator as computed by `pense::pense()`:
```{r mmridge4}
g <- mstep(b0, complete_grid=TRUE)
cbind(a$coef, as.vector(b0$coef[,1]), g$coefficients[,1])
```
The scale estimators are `r c(a$scale, b0$scale, g$scale)`, and
the optimal value of the penalization found in this M-step
was `r g$lambda`.


## Nevertheless, things can be rather different sometimes

If we repeat the same experiment as above, but with `n = 50` instead of `n = 100`
we see that the two implementations of S-ridge can be rather different:
```{r round2, message=FALSE, warning=FALSE}
# another simple synthetic example
n <- 50
p <- 20
set.seed(123)
x <- matrix(rnorm(n*p), n, p)
y <- as.vector( x %*% c(rep(2, 5), rep(0, p-5))) + rnorm(n, sd=.5)
# mmlasso S-ridge
a <- sridge(x=x, y=y, cualcv.S=5, numlam.S=30, niter.S=50, normin=0,
            denormout=0, alone=1, ncores=4)
# pense S-ridge 
b0 <- pense(X=x, y=y, alpha=0, standardize=TRUE, lambda=1e-9, initial='cold',
           options=pense_options(delta=a$delta))
```
The optimal penalization from `sridge` is still `r a$lamda`. 
The scales and regression coefficients:
```{r scales2}
c(a$scale, b0$scale)
cbind(a$coef, as.vector(b0$coef[,1]))
```
Not surprisingly, the difference carries over to the M-step:
```{r mmridge2}
g <- mstep(b0, complete_grid=TRUE)
c(a$scale, b0$scale, g$scale)
cbind(a$coef, as.vector(b0$coef[,1]), g$coefficients[,1])
```


## Something weird for large sample sizes?

If we repeat the same experiment as above, but with `n = 500`, we see that
the difference between the `mmlasso` and `pense` implementations of S-ridge
remain in place, but intriguingly, the residual scale estimator reported 
by `pense::mstep` is **now the same as** the one returned by `pense::pense`.
```{r round3, message=FALSE, warning=FALSE}
# another simple synthetic example
n <- 500
p <- 20
set.seed(123)
x <- matrix(rnorm(n*p), n, p)
y <- as.vector( x %*% c(rep(2, 5), rep(0, p-5))) + rnorm(n, sd=.5)
# mmlasso S-ridge
a <- sridge(x=x, y=y, cualcv.S=5, numlam.S=30, niter.S=50, normin=0,
            denormout=0, alone=1, ncores=4)
# pense S-ridge 
b0 <- pense(X=x, y=y, alpha=0, standardize=TRUE, lambda=1e-9, initial='cold',
           options=pense_options(delta=a$delta))
```
The optimal penalization from `sridge` is still `r a$lamda`. 
The scales and regression coefficients:
```{r scales3}
c(a$scale, b0$scale)
cbind(a$coef, as.vector(b0$coef[,1]))
```
Not surprisingly, the difference carries over to the M-step:
```{r mmridge3}
g <- mstep(b0, complete_grid=TRUE)
c(a$scale, b0$scale, g$scale)
cbind(a$coef, as.vector(b0$coef[,1]), g$coefficients[,1])
```

